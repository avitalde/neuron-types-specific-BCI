{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd1fb91-f64f-4cec-a327-db07f81475cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc0aec-8fac-41c4-a165-1c1c23a54e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import cylouvain\n",
    "import networkx as nx\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import matplotlib as mpl\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from scipy import linalg\n",
    "import copy\n",
    "import umap\n",
    "\n",
    "#!git clone https://github.com/NeuralAnalysis/PyalData\n",
    "from PyalData.pyaldata import *\n",
    "\n",
    "\n",
    "# for LSTM\n",
    "import xarray as xr\n",
    "from Neural_Decoding.decoders import LSTMDecoder\n",
    "from Neural_Decoding.decoders import LSTMClassification\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "# for Bayes Classifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49a026-c05b-41db-a5eb-c5eebb294de8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88305b1-2a1c-4ad5-b23c-940529c2e1ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d998db4e-4947-400f-accb-e5e0b0673fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset_from_mat(file_path: str):\n",
    "    '''\n",
    "    @arg file_path - path to file in .mat format\n",
    "    @returns data_pd - dataset as pandas DataFrame  \n",
    "    '''\n",
    "\n",
    "    data_pd = mat2dataframe(file_path, shift_idx_fields=True)\n",
    "    return data_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245a81b-3af9-4aa8-96b6-81c8b1b9536c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing - waveform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64004136-a09e-4977-965d-4f0a8f112cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trial_ID(data_waveform):\n",
    "    '''\n",
    "    remove units (neurons) with ID 0 or ID 255 from waveform dataset (depends on software used to record)\n",
    "    \n",
    "    @arg data_waveform - waveform dataset, type pandas DataFrame\n",
    "    @return filtered_data - filtered waveform dataset\n",
    "    '''\n",
    "    trials_to_keep = []\n",
    "    for i in range(data_waveform.shape[0]):\n",
    "        if(data_waveform.ID[i] != 0 and data_waveform.ID[i] != 255): \n",
    "            if np.ndim(data_waveform.spikes.iloc[i]) == 1: print(i) # to identify bugs\n",
    "            if data_waveform.chan[i] > 96: print(i) # to identify bugs\n",
    "            trials_to_keep.append(i)\n",
    "   \n",
    "    waveform_filtered = data_waveform.loc[trials_to_keep, :] \n",
    "    return waveform_filtered\n",
    "\n",
    "\n",
    "def filter_SNR(waveform_filtered, data_spikes, SNR):\n",
    "    '''\n",
    "    filter waveform dataset, keep only waveforms with minimal SNR\n",
    "    \n",
    "    @arg waveform_filtered - waveform dataset, type pandas DataFrame\n",
    "    @return waveform_filtered - filtered waveform dataset\n",
    "    @return trials_to_keep_M1 - list containing index of trials belonging to M1 neurons\n",
    "    @return trials_to_keep_PMd - list containing index of trials belonging to PMd neurons\n",
    "    '''\n",
    "    \n",
    "    trials_to_keep = []\n",
    "    trials_to_keep_M1 = []\n",
    "    trials_to_keep_PMd = []\n",
    "\n",
    "    for j in range(waveform_filtered.shape[0]): # for each neuron (each row)\n",
    "        b = []\n",
    "        for i in range(waveform_filtered.spikes.iloc[j].shape[0]): # for each spike from neuron j (each row in spikes = waveform)\n",
    "            AP = waveform_filtered.spikes.iloc[j][i,:] # save here the AP to make it faster\n",
    "            peak = np.amax(waveform_filtered.spikes.iloc[j][i,1:])\n",
    "            trough = np.min(AP)\n",
    "            std = np.std(AP)\n",
    "            a = (peak - trough)/std # snr of each spike from neuron j\n",
    "            b.append(a) # [snr_spike0, snr_spike1, snr_spike2, ..., snr_spikeEND]\n",
    "        b = np.array(b)\n",
    "        snr = np.mean(b) # average SNR for neuron j\n",
    "\n",
    "        if(snr > SNR):# if snr of neuron j is > SNR:\n",
    "            trials_to_keep.append(j) # save index of neurons to keep\n",
    "\n",
    "            if j <= data_spikes.M1_spikes[0].shape[1]-1: # j=neuron_ID <= number of neurons in M1 in trial\n",
    "                trials_to_keep_M1.append(j) # to compare with M1_unit_guide\n",
    "            else:\n",
    "                trials_to_keep_PMd.append(j-data_spikes.M1_spikes[0].shape[1])#it was shape[1]-1 # to compare with PMd_unit_guide\n",
    "\n",
    "    waveform_filtered = waveform_filtered.iloc[trials_to_keep, :] # same structure than original waveform dataset but only neurons with snr>3\n",
    "        \n",
    "    return waveform_filtered, trials_to_keep_M1, trials_to_keep_PMd\n",
    "\n",
    "\n",
    "def concatenate_average_trials(waveform_filtered):\n",
    "    '''\n",
    "    concatenate average of each trial\n",
    "    \n",
    "    @arg data - waveform dataset, type pandas DataFrame, already filtered\n",
    "    @return concatenated_avg - concatenated array\n",
    "    '''\n",
    "    \n",
    "    concatenate_avg = []\n",
    "    for j in range(waveform_filtered.shape[0]): # for each neuron\n",
    "        avg = np.mean(waveform_filtered.spikes.iloc[j], axis=0) # average among all waveforms across time (= across all rows) -> each channel has only the average spike\n",
    "        b = np.array(avg)\n",
    "        b = np.delete(b,0)  \n",
    "        concatenate_avg.append(b) # concatenate all averaged spikes [[average spike of channels 0], [average spike of channels 1], ...]\n",
    "    concatenate_avg = np.array(concatenate_avg)\n",
    "    \n",
    "    return concatenate_avg\n",
    "\n",
    "\n",
    "def normalize_data(concatenated_data, plot=False):\n",
    "    '''\n",
    "    Normalize each waveform between -1 and 1\n",
    "    \n",
    "    @arg concatenated_data - concatenated waveforms from all trials\n",
    "    @return data_normalized - list of all normalized waveforms\n",
    "    '''\n",
    "    data_normalized = []\n",
    "\n",
    "    for i in range(concatenated_data.shape[0]):\n",
    "        wave = concatenated_data[i]\n",
    "\n",
    "        d = 2.*(wave - np.min(wave))/np.ptp(wave)-1 # normalization between -1 and 1 of each waveform\n",
    "        data_normalized.append(d)\n",
    "        \n",
    "        if plot:\n",
    "            plt.plot(data_normalized[i], c = 'blue')\n",
    "\n",
    "    if plot:\n",
    "        plt.title('Normalized waveform with respect to the neuron activity')\n",
    "        plt.ylim((-1.2,1.2))\n",
    "        plt.show()\n",
    "    \n",
    "    return data_normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ef02f-7173-4ade-982c-752b7b4ea7ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing - spikes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807e275-e460-48bb-904f-bdf8446c52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_general(data_spikes, combine=True, std=0.05):\n",
    "    '''\n",
    "    process spike data, the following steps are done:\n",
    "        - combine bins\n",
    "        - sqrt transform\n",
    "        - filter only successful triasl\n",
    "        - convert firing rates column\n",
    "        \n",
    "    @arg data_spikes - firing data, type: pandas DataFrame \n",
    "    @arg combine - Bool, if True, combine bins\n",
    "    @arg std - std defined for smoothing function\n",
    "    \n",
    "    @return processed firing data, type: pandas DataFrame \n",
    "    '''\n",
    "    if combine:\n",
    "        td = combine_time_bins(data_spikes, 2) #merge 2 bins -> 20 ms bins\n",
    "    else:\n",
    "        td = data_spikes\n",
    "    td = transform_signal(td, \"M1_spikes\",  'sqrt')\n",
    "    td = transform_signal(td, \"PMd_spikes\", 'sqrt')\n",
    "    td = merge_signals(td, [\"M1_spikes\", \"PMd_spikes\"], \"both_spikes\")\n",
    "    td = add_firing_rates(td, 'smooth', std=std) # add M1_rates and PMd_rates fields -> rate = spikes/bin_size\n",
    "    td = select_trials(td, \"result == 'R'\")\n",
    "    return td\n",
    "\n",
    "def preprocessing_spikes(td, trials_to_keep_M1, trials_to_keep_PMd, start_point_name, rel_start, rel_end, rates=True):\n",
    "    '''\n",
    "    preprocess 2\n",
    "    \n",
    "    @arg td - preprocessed firing data, type: pandas DataFrame \n",
    "    @arg trials_to_keep_M1 - list of neurons to keep from M1\n",
    "    @arg trials_to_keep_PMd - list of neurons to keep from PMd\n",
    "    @arg rates - if True, subtract cross condition mean (because this filtered dataset will be used to work with rates)\n",
    "    @arg rel_start, rel_end - when to start/end extracting relative to the starting time point (restrict interval)\n",
    "    @start_point_name - name of the time point around which the interval starts\n",
    "    \n",
    "    @return processed and filtered firing data, type: pandas DataFrame \n",
    "    '''\n",
    "    \n",
    "    td_processed = restrict_to_interval(td, start_point_name, rel_start=rel_start, rel_end=rel_end)\n",
    "    \n",
    "    # this part of preprocessing depends on if we look at spikes or at rates\n",
    "    if rates:\n",
    "        td_processed = subtract_cross_condition_mean(td_processed) #Find mean across all trials for each time point and subtract it from each trial.\n",
    "    \n",
    "    # only save data of neurons that had SNR >3 (found before)\n",
    "    for j in range(td_processed.shape[0]):\n",
    "        td_processed.M1_spikes[j] = td_processed.M1_spikes[j][:, trials_to_keep_M1] # all rows corresponding to neurons with SNR>3\n",
    "        td_processed.PMd_spikes[j] = td_processed.PMd_spikes[j][:, trials_to_keep_PMd] # all rows corresponding to neurons with SNR>3\n",
    "\n",
    "    # unit_guide[i] has shape #neurons x 2, ex M1_unit_guide -> 76x2 (dataset 1)\n",
    "    # why this only for first trial and not for all of the trials?\n",
    "    td_processed.M1_unit_guide[0] = td_processed.M1_unit_guide[0][trials_to_keep_M1, :] # only get the neurons (both columns) with SNR >3\n",
    "    td_processed.PMd_unit_guide[0] = td_processed.PMd_unit_guide[0][trials_to_keep_PMd, :] # only get the neurons (both columns) with SNR >3\n",
    "    \n",
    "    td_processed = remove_low_firing_neurons(td_processed, \"M1_spikes\", 1)\n",
    "    td_processed = remove_low_firing_neurons(td_processed, \"PMd_spikes\", 1)\n",
    "    td_processed[\"target_id\"] = td_processed.apply(get_target_id, axis=1)\n",
    "    \n",
    "    return td_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3af28-d166-48e4-9af4-31c08988581c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# UMAP + Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39228f2b-5e38-448c-a534-bc530a6386fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_on_waveform(data_normalized, neighbors, seed, n_components=2, plot=True):\n",
    "    '''\n",
    "    perform UMAP\n",
    "    \n",
    "    @arg data_normalized - normalised waveform dataset, type pandas DataFrame\n",
    "    @arg neighbors_ - number of neighbours to perform UMAP\n",
    "    @arg seed_ - random seed for UMAP\n",
    "    @arg n_components - number of dimension for resulting UMAP graph\n",
    "    @arg plot - if True plot UMAP graph \n",
    "    \n",
    "    @return umap_df - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                      graph and corresponding x and y position\n",
    "    @return mapper - UMAP output (needed for Louvian clustering)\n",
    "    @return embedding - array, eavh row is a neuron represented in the UMAP graph\n",
    "    \n",
    "    '''\n",
    "    # UMAP\n",
    "    reducer = umap.UMAP(n_neighbors = neighbors, min_dist=0.1, random_state=seed, n_components=n_components) # old -> n_neighbors=30\n",
    "    mapper = reducer.fit(data_normalized)\n",
    "    embedding = reducer.transform(data_normalized)\n",
    "\n",
    "    if plot:\n",
    "        # Each row of the embedding array is a 2D/3D representation of the corresponding unit(=neuron)\n",
    "        if n_components == 3:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.scatter(embedding[:,0], embedding[:,1], embedding[:,2], s=20,  c = '#75759E')\n",
    "        else:\n",
    "            plt.scatter(embedding[:,0], embedding[:,1], s=20,  c = '#75759E')\n",
    "\n",
    "    # save all normalised data into a DataFrame with corresopnding position in umap\n",
    "    if n_components == 3:\n",
    "        umap_df = pd.DataFrame(embedding, columns=('x', 'y', 'z')) # create DataFrame with 2 columns: x,y coordinates of UMAP diagram\n",
    "    else:\n",
    "        umap_df = pd.DataFrame(embedding, columns=('x', 'y')) # create DataFrame with 2 columns: x,y coordinates of UMAP diagram        \n",
    "    umap_df['waveform'] = list(data_normalized) # add to dataframe a column with waveform corresponding to that point\n",
    "    \n",
    "    #umap.plot.connectivity(mapper, show_points=True)\n",
    "    return umap_df, mapper, embedding\n",
    "\n",
    "\n",
    "def louvian_clustering(mapper, umap_df, CUSTOM_PAL_SORT, res, n_components=2, plot=True):\n",
    "    '''\n",
    "    perform Louvain Community Detection\n",
    "    \n",
    "    @arg mapper - UMAP output, type umap.umap_.UMAP\n",
    "    @arg umap_df - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                      graph and corresponding x and y position\n",
    "    @arg CUSTOM_PAL_SORT - list of colors for each cluser\n",
    "    @arg res - resolution for Louvian clustering\n",
    "    @arg n_components - number of dimension from UMAP graph\n",
    "    @arg plot - if True plot UMAP graph \n",
    "    \n",
    "    @return umap_df - pandas DataFrame same as input but with added cluster each point belongs to\n",
    "    @return clustering_solution - list of clusters each neuron belongs to\n",
    "    '''\n",
    "    # Louvian clustering\n",
    "    G = nx.from_scipy_sparse_matrix(mapper.graph_) # build graph, every node is a points on UMAP graph\n",
    "    clustering = cylouvain.best_partition(G, resolution = res) # returns dict {'node': cluster it belongs to}\n",
    "    clustering_solution = list(clustering.values()) # number of community (cluster) of each node\n",
    "    umap_df['cluster'] = clustering_solution\n",
    "    \n",
    "    if plot:\n",
    "        cluster_colors = [CUSTOM_PAL_SORT[i] for i in clustering_solution]\n",
    "        if n_components == 3:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.scatter(umap_df['x'].tolist(), umap_df['y'].tolist(),umap_df['z'].tolist(), marker='o', c=cluster_colors, s=20,linewidth=0.5)\n",
    "        else:\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            plt.scatter(umap_df['x'].tolist(), umap_df['y'].tolist(), marker='o', c=cluster_colors, s=50,linewidth=0.5)\n",
    "    \n",
    "    return umap_df, clustering_solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338af764-2326-45f1-80d8-46771db7a332",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Functions - general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea01789-f01c-470c-80b2-d9ed51fe22ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only used to check difference with previous results\n",
    "def neurons_in_cluster(clustering_solution):\n",
    "    '''\n",
    "    compute number of neurons in each cluster\n",
    "    \n",
    "    @arg clustering_solution - list of clusters each neuron belongs to\n",
    "    \n",
    "    @return neurons - list of number of neurons in each class\n",
    "    '''\n",
    "    neurons = [0]*(max(clustering_solution)+1)\n",
    "\n",
    "    for j in range(max(clustering_solution)+1): # for each cluster\n",
    "        for i in range(len(clustering_solution)): #for each neuron=node in UMAP graph\n",
    "            if(clustering_solution[i] == j): # count neurons in cluster 7\n",
    "                  neurons[j] += 1\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64a34a-0b72-4ba6-8e75-5892d5ac0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_classes(clustering_solution):\n",
    "    '''\n",
    "    assign number (ID) of neuron to the class it belongs to\n",
    "    \n",
    "    @arg clustering_solution - list of clusters each neuron belongs to\n",
    "    \n",
    "    @return neuron_types - dict: key is cluster, values are list with all neurons belonging to that cluster\n",
    "    '''\n",
    "    neuron_types = {}\n",
    "    for i in range(len(set(clustering_solution))):\n",
    "        type_i = []\n",
    "        for j in range(len(clustering_solution)):\n",
    "            if(clustering_solution[j] == i):\n",
    "                type_i.append(j)\n",
    "        neuron_types[i] = type_i\n",
    "    return neuron_types\n",
    "\n",
    "\n",
    "\n",
    "def sample_neurons(neuron_types, N_samples):\n",
    "    '''\n",
    "    sample ranodm neurons from each cluster\n",
    "    \n",
    "    @arg neuron_types - dict: key is cluster, values are list with all neurons belonging to that cluster\n",
    "    @arg N_samples - number of neurons we want to sample from each cluster\n",
    "    \n",
    "    @return sampled_data - dict: key is cluster, values are list with sampled neurons belonging to that cluster\n",
    "    '''\n",
    "    # neuron_types: dictionary -> {cluster_number: [id=number of all neurons in this cluster], ...}\n",
    "    # N_samples: number of neurons to sample from each neuron-class\n",
    "    neuron_types_copy = copy.deepcopy(neuron_types)\n",
    "    sampled_data = {}\n",
    "    for i in range(len(neuron_types)): # for each cluster\n",
    "        type_index = []\n",
    "        for j in range(N_samples): # select x neurons from each cluster\n",
    "            random_neuron = random.choice(neuron_types_copy[i]) # pick a random neuron from cluster i\n",
    "            type_index.append(random_neuron)\n",
    "            neuron_types_copy[i].remove(random_neuron) # remove already picked neuron so they don't appear twice\n",
    "        sampled_data[i] = type_index[0:N_samples] # 22 neurons for each cluster\n",
    "\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f7af5-2d51-4535-be95-0241fe3055e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Cluster Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d51c8-b1ba-4fe1-ad4b-21fc6f42d149",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00b871-d83a-4c72-93ec-0a1128169051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_variance_explained(td_processed, neuron_types, vline, CUSTOM_PAL_SORT, hline=True):\n",
    "    '''\n",
    "    plot variance explained ratio as a function of PCA number of dimensions for each cluster\n",
    "    \n",
    "    @arg td_processed - preprocessed and filtered firing data\n",
    "    @arg neuron_types - dict: key is cluster, values are list with all neurons belonging to that cluster\n",
    "    @arg vline - where to plot vertical line on graph\n",
    "    @arg CUSTOM_PAL_SORT - personalised colour palette\n",
    "    @arg hline - if True plot horizontal line corresponding to 85% variance explained\n",
    "    '''\n",
    "\n",
    "    for j in range(len(neuron_types)):\n",
    "        data_spikes_new = merge_signals(td_processed, [\"M1_rates\", \"PMd_rates\"], \"both_rates\")\n",
    "\n",
    "        for i in range(data_spikes_new.shape[0]):\n",
    "            data_spikes_new.both_rates[i] = data_spikes_new.both_rates[i][:, neuron_types[j]]\n",
    "\n",
    "        # PCA\n",
    "        pca_dims = len(neuron_types[j])\n",
    "        pca = fit_dim_reduce_model(data_spikes_new, PCA(pca_dims), \"both_rates\")\n",
    "\n",
    "        for i in range(1, len(pca.explained_variance_ratio_ )):\n",
    "            pca.explained_variance_ratio_[i] = pca.explained_variance_ratio_[i] + pca.explained_variance_ratio_[i-1]\n",
    "\n",
    "        #t = list(range(1,pca.explained_variance_ratio_.shape[0]+1))\n",
    "        #plt.plot(t,pca.explained_variance_ratio_, c= CUSTOM_PAL_SORT[j], label=f'{j}')\n",
    "        t = list(range(0,pca.explained_variance_ratio_.shape[0]+1))\n",
    "        plt.plot(t,np.insert(pca.explained_variance_ratio_,0,0), c= CUSTOM_PAL_SORT[j], label=f'{j}')\n",
    "        plt.scatter(t,np.insert(pca.explained_variance_ratio_,0,0),marker='.', facecolors='none', edgecolors= CUSTOM_PAL_SORT[j], s=25)\n",
    "    \n",
    "    max_line = max([len(neuron_types[i]) for i in range(len(neuron_types))])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Number of dimensions')\n",
    "    plt.ylabel('Variance explained')\n",
    "    plt.vlines(vline, 0, 1, color='k', linewidth=1, linestyle='--')\n",
    "    if hline:\n",
    "        plt.hlines(0.85, 0, max_line, color='k', linewidth=1, linestyle='--')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "def apply_PCA(td_processed, sampled_data, cluster, pca_dims, return_model=False):\n",
    "    '''\n",
    "    apply PCA to a sub-sampled set of data of a specific neuron cluster\n",
    "    \n",
    "    @arg td_processed - processed and filtered firing data, type: pandas DataFrame \n",
    "    @arg sampled_data - sampled neurons from each cluster\n",
    "    @arg cluster - (int) cluster to apply PCA on.\n",
    "                    If cluster=-1: sampled_data contains only sampled neurons \n",
    "    @arg pca_dims - number of dimensions for PCA\n",
    "    @return_model - if True return also model\n",
    "    \n",
    "    @return move_td - dataset (pandas DataFrame) with added field with pca output\n",
    "    @return model - dimensionality reduction model\n",
    "    '''\n",
    "    \n",
    "    if cluster != -1:\n",
    "        sampled_data = sampled_data[cluster]\n",
    "        \n",
    "    move_td3 = merge_signals(td_processed, [\"M1_rates\", \"PMd_rates\"], \"both_rates\")\n",
    "    for i in range(move_td3.shape[0]):\n",
    "        move_td3.both_rates[i] = move_td3.both_rates[i][:, sampled_data]\n",
    "\n",
    "    # apply PCA\n",
    "    if return_model:\n",
    "        move_td, model = dim_reduce(move_td3, PCA(pca_dims), \"both_rates\", \"both_pca\", return_model=return_model)\n",
    "        return move_td, model\n",
    "    else:\n",
    "        move_td = dim_reduce(move_td3, PCA(pca_dims), \"both_rates\", \"both_pca\", return_model=return_model)\n",
    "        return move_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e39bc-02e0-496a-9322-a9a16f9bbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used\n",
    "def orthogonality_prep_mov(td_processed, sampled_data, cluster, pca_dims=10):\n",
    "    \n",
    "    data_spikes_merged = merge_signals(td_processed, [\"M1_rates\", \"PMd_rates\"], \"both_rates\")\n",
    "    for i in range(data_spikes_merged.shape[0]):\n",
    "          data_spikes_merged.both_rates[i] = data_spikes_merged.both_rates[i][:, sampled_data[cluster]]\n",
    " \n",
    "    pca = fit_dim_reduce_model(data_spikes_merged, PCA(pca_dims), \"both_rates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe9513-ff98-44c0-b512-ad2bacad7bca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## dPCA - not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2f289-11c8-4444-9115-4563bcdaf078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not used\n",
    "def apply_dPCA(td_processed, tr_per_target, N_samples, cluster):\n",
    "    '''\n",
    "    apply dPCA to a sub-sampled set of data from all trials and targets\n",
    "    \n",
    "    @arg td_processed - processed and filtered firing data, type: pandas DataFrame \n",
    "    @arg tr_per_target - 2D array containing trials id of each trial going to each specific target\n",
    "    @arg N_samples - number of neurons to sample from each cluster\n",
    "    @arg cluster - (int) cluster to apply dPCA on.\n",
    "                    If cluster=-1: sampled neurons from all dataset, not from one cluster only\n",
    "    @return \n",
    "    \n",
    "    '''\n",
    "    num_trials = td_processed.shape[0]\n",
    "    num_neurons = td_processed.both_spikes[0].shape[1]\n",
    "    num_targets = len(tr_per_target)\n",
    "    min_per_target = np.min(tr_per_target)\n",
    "    num_time = td_processed.both_spikes[0].shape[0]\n",
    "\n",
    "     \n",
    "    if cluster == -1:\n",
    "        sampled_data = np.random.choice(np.arange(num_neurons), size=N_samples) # sample from whole dataset\n",
    "    else:\n",
    "        sampled_data = sample_neurons(neuron_types, N_samples)\n",
    "        \n",
    "    data = np.zeros((min_per_target, N_samples, num_targets, num_time))\n",
    "    tr_per_target_found = [0]*num_targets\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        targetID = td_processed.target_id[trial] # ID of target of this trial\n",
    "        if tr_per_target_found[targetID] >= min_per_target: continue\n",
    "        data[tr_per_target_found[targetID], :, targetID, :] = np.transpose(td_processed.both_spikes[trial][:,sampled_data[cluster]])\n",
    "        tr_per_target[targetID] += 1 # trials for this target already found\n",
    "\n",
    "    avg_trial_per_target = np.mean(data, axis=0)\n",
    "    \n",
    "    dpca = dPCA.dPCA(labels='st',regularizer='auto')\n",
    "    dpca.protect = ['t']\n",
    "    Z = dpca.fit_transform(avg_trial_per_target, data)\n",
    "    \n",
    "    return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f0fc7a-867b-46a3-b89b-b0ab95bf4afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not used\n",
    "def dPCA_transformation_matrix(td_processed, tr_per_target, sampled_data, N_samples, cluster):\n",
    "    '''\n",
    "    .........\n",
    "    '''\n",
    "    # sample neurons\n",
    "\n",
    "    # we want to reshape data according to the following:\n",
    "    # [num of trials in each target, num of neurons, num of targets, time point]\n",
    "    num_trials = td_processed.shape[0]\n",
    "    num_neurons = td_processed.both_spikes[0].shape[1]\n",
    "    num_targets = len(tr_per_target)\n",
    "    min_per_target = np.min(tr_per_target)\n",
    "    num_time = td_processed.both_spikes[0].shape[0]\n",
    "\n",
    "    #neuron_types = neuron_classes(clustering_solution) # neuron in each cluster\n",
    "\n",
    "    Z_all = {}\n",
    "    transf_matrix_t = {}\n",
    "    transf_matrix_s = {}\n",
    "    transf_matrix_st = {}\n",
    "    for clust in range(len(neuron_types)):\n",
    "            Z_all[f'{clust}'] = []\n",
    "            transf_matrix_t[f'{clust}'] = []\n",
    "            transf_matrix_s[f'{clust}'] = []\n",
    "            transf_matrix_st[f'{clust}'] = []\n",
    "\n",
    "    #for cluster in range(len(neuron_types)):\n",
    "\n",
    "    #sampled_data = sample_neurons(neuron_types, N_samples)\n",
    "    data = np.zeros((min_per_target, N_samples, num_targets, num_time))\n",
    "    tr_per_target_found = [0]*num_targets\n",
    "\n",
    "    # reshape data to use it in the dPCA function\n",
    "    for trial in range(num_trials):\n",
    "        targetID = td_processed.target_id[trial] # ID of target of this trial\n",
    "        if tr_per_target_found[targetID] >= min_per_target: continue\n",
    "        data[tr_per_target_found[targetID], :, targetID, :] = np.transpose(td_processed.both_spikes[trial][:,sampled_data[cluster]])\n",
    "        tr_per_target_found[targetID] += 1 # trials for this target already found\n",
    "    avg_trial_per_target = np.mean(data, axis=0)\n",
    "\n",
    "    # apply dPCA \n",
    "    dpca = dPCA.dPCA(labels='st',regularizer='auto')\n",
    "    dpca.protect = ['t']\n",
    "    Z_all = dpca.fit_transform(avg_trial_per_target, data)\n",
    "\n",
    "    # compute inverse of avg data\n",
    "    data_avg_inv = np.linalg.pinv(avg_trial_per_target)\n",
    "\n",
    "    '''\n",
    "    # compute transformation matrices for each cluster\n",
    "    transf_matrix_t[f'{cluster}'] = np.matmul(np.transpose(data_avg_inv,[2,0,1]),np.transpose(Z_all[f'{cluster}']['t'],[1,2,0]))\n",
    "    transf_matrix_s[f'{cluster}'] = np.matmul(np.transpose(data_avg_inv,[2,0,1]),np.transpose(Z_all[f'{cluster}']['s'],[1,2,0]))\n",
    "    transf_matrix_st[f'{cluster}'] = np.matmul(np.transpose(data_avg_inv,[2,0,1]),np.transpose(Z_all[f'{cluster}']['st'],[1,2,0]))\n",
    "    '''\n",
    "    transf_matrix_t = np.matmul(np.transpose(data_avg_inv,[2,0,1]),np.transpose(Z_all['t'],[1,2,0]))\n",
    "    transf_matrix_s = np.matmul(np.transpose(data_avg_inv,[2,0,1]),np.transpose(Z_all['s'],[1,2,0]))\n",
    "    transf_matrix_st = np.matmul(np.transpose(data_avg_inv,[2,0,1]),np.transpose(Z_all['st'],[1,2,0]))\n",
    "    \n",
    "    return transf_matrix_t, transf_matrix_s, transf_matrix_st\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f3d70-f168-42b0-a025-b4a2e63f7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used\n",
    "def dpca_explained_variance(td_processed, tr_per_target, neuron_types, n_components):\n",
    "    # all neurons from each cluster\n",
    "\n",
    "    # we want to reshape data according to the following:\n",
    "    # [num of trials in each target, num of neurons, num of targets, time point]\n",
    "    num_trials = td_processed.shape[0]\n",
    "    num_neurons = td_processed.both_spikes[0].shape[1]\n",
    "    num_targets = len(tr_per_target)\n",
    "    min_per_target = np.min(tr_per_target)\n",
    "    num_time = td_processed.both_spikes[0].shape[0]\n",
    "\n",
    "    Z_all = {}\n",
    "    explained_variance_ratio = {}\n",
    "    \n",
    "    for clust in range(len(neuron_types)):\n",
    "            Z_all[f'{clust}'] = []\n",
    "            explained_variance_ratio[f'{clust}'] = []\n",
    "\n",
    "    for cluster in range(len(neuron_types)):\n",
    "\n",
    "        data = np.zeros((min_per_target, len(neuron_types[cluster]), num_targets, num_time))\n",
    "        tr_per_target_found = [0]*num_targets\n",
    "\n",
    "        for trial in range(num_trials):\n",
    "            targetID = td_processed.target_id[trial] # ID of target of this trial\n",
    "            if tr_per_target_found[targetID] >= min_per_target: continue\n",
    "            data[tr_per_target_found[targetID], :, targetID, :] = np.transpose(td_processed.both_spikes[trial][:,neuron_types[cluster]])\n",
    "            tr_per_target_found[targetID] += 1 # trials for this target already found\n",
    "\n",
    "        avg_trial_per_target = np.mean(data, axis=0)\n",
    "\n",
    "        # apply dPCA \n",
    "        dpca = dPCA.dPCA(labels='st',regularizer='auto', n_components=n_components)\n",
    "        dpca.protect = ['t']\n",
    "        \n",
    "        Z_all[f'{cluster}'] = dpca.fit_transform(avg_trial_per_target, data)\n",
    "        explained_variance_ratio[f'{cluster}'] = dpca.explained_variance_ratio_\n",
    "    \n",
    "    return explained_variance_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa98bf7-43dc-4a7e-89c7-9f727955a46c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c67b5b0-cfa8-4e65-b28e-f4f5bd03bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_id(trial):\n",
    "    '''\n",
    "    convert direction from dataset to target ID \n",
    "    \n",
    "    @arg trial - number of trial from dataset (int)\n",
    "    \n",
    "    @return target ID (int)\n",
    "    '''\n",
    "    return int(np.round((trial.target_direction + np.pi) / (0.25 * np.pi))) - 1\n",
    "\n",
    "\n",
    "def trials_per_target(td_processed):\n",
    "    '''\n",
    "    select trials going to each target\n",
    "    \n",
    "    @arg td_processed - preprocessed and filtered firing data\n",
    "    \n",
    "    @return 2D list, each entry is a list of all trials going to specific target\n",
    "    '''\n",
    "    targets = td_processed.target_id.unique() # [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    targets.sort(axis=0)  # ascending order\n",
    "\n",
    "    # separate by target\n",
    "    # each list contains indices of trials going to target __\n",
    "    target_0_index = []\n",
    "    target_1_index = []\n",
    "    target_2_index = []\n",
    "    target_3_index = []\n",
    "    target_4_index = []\n",
    "    target_5_index = []\n",
    "    target_6_index = []\n",
    "    target_7_index = []\n",
    "\n",
    "    for i in range(td_processed.shape[0]):\n",
    "        if(td_processed.target_id[i] == 0):\n",
    "            target_0_index.append(i)\n",
    "        if(td_processed.target_id[i] == 1):\n",
    "            target_1_index.append(i)\n",
    "        if(td_processed.target_id[i] == 2):\n",
    "            target_2_index.append(i)\n",
    "        if(td_processed.target_id[i] == 3):\n",
    "            target_3_index.append(i)\n",
    "        if(td_processed.target_id[i] == 4):\n",
    "            target_4_index.append(i)\n",
    "        if(td_processed.target_id[i] == 5):\n",
    "            target_5_index.append(i)\n",
    "        if(td_processed.target_id[i] == 6):\n",
    "            target_6_index.append(i)\n",
    "        if(td_processed.target_id[i] == 7):\n",
    "            target_7_index.append(i)\n",
    "\n",
    "    # [[indices of trials going to taregt 1], [indices of trials going to taregt 2],...]\n",
    "    target_index = [target_0_index, target_1_index, target_2_index, target_3_index, target_4_index, target_5_index, target_6_index, target_7_index]\n",
    "    \n",
    "    return target_index\n",
    "\n",
    "\n",
    "def get_lat_variables_per_target(target, data, dim, target_index):\n",
    "    '''\n",
    "    get latent variables for each target -> average response of all neurons when going to specific target\n",
    "    \n",
    "    @arg target - number of target (int from 0 to 7)\n",
    "    @arg data - firing rates output from PCA\n",
    "    @arg dim - slected number of dimensions for PCA\n",
    "    \n",
    "    @return latent variable, 2D list of size _time_interval_ x _PCA_dimensions_\n",
    "    \n",
    "    '''\n",
    "    time_interval = data[0].shape[0] # time-steps left after applying combine_time_bins() and restrict_to_interval()\n",
    "    sum_per_target = np.zeros((time_interval, dim)) \n",
    "\n",
    "    for trial in target_index[target]: # for each trial going to given target \n",
    "        sum_per_target  = sum_per_target  + data[trial] # sum \n",
    "\n",
    "    latent_variable = sum_per_target /len(target_index[target]) # compute average response for this target\n",
    "    \n",
    "    return latent_variable \n",
    "\n",
    "\n",
    "def get_all_latent_variables(td_processed, neuron_types, pca_dims, target_index):\n",
    "    '''\n",
    "    @arg neuron_types - dict: key is cluster, values are list with all neurons belonging to that cluster\n",
    "    @arg td_processed - preprocessed and filtered firing rates data\n",
    "    @arg pca_dims - number of dimensions chosen for PCA (should change for each cluster?)\n",
    "    @ arg target_index - 2D list, each entry is a list of all trials going to specific target\n",
    "    \n",
    "    @return dict: key is cluster, values are latent variables for all targets for that class\n",
    "    '''\n",
    "    \n",
    "    clusters = len(neuron_types)\n",
    "    num_targets = np.shape(target_index)[0]\n",
    "    \n",
    "    lat_variables = {} # key is cluster number, values are latent variables for that cluster for all targets\n",
    "    for clust in range(clusters):\n",
    "        lat_variables[f'{clust}'] = []\n",
    "\n",
    "    for clust in range(clusters):\n",
    "        spikes_data = merge_signals(td_processed, [\"M1_rates\", \"PMd_rates\"], \"both_rates\")\n",
    "        for i in range(spikes_data.shape[0]):   \n",
    "            spikes_data.both_rates[i] = spikes_data.both_rates[i][:, neuron_types[clust]]\n",
    "\n",
    "        # PCA\n",
    "        spikes_data = dim_reduce(spikes_data, PCA(pca_dims), \"both_rates\", \"both_pca\")\n",
    "        \n",
    "        for target in range(num_targets):  \n",
    "            lat_variables[f'{clust}'].append(get_lat_variables_per_target(target, spikes_data.both_pca, pca_dims, target_index))\n",
    "\n",
    "    return lat_variables\n",
    "\n",
    "\n",
    "def plot_latent_variables(lat_variables, target_index, cluster, l_var, ax):\n",
    "    '''\n",
    "    plot one latent variable for a specific cluster\n",
    "   \n",
    "    \n",
    "    @arg lat_variables - dictwith all latent variables:\n",
    "                        key is cluster, values are latent variables for all targets for that class.\n",
    "    @arg cluster - cluster of latent variable to plot\n",
    "    @arg l_var - specific latent variable to be plotted\n",
    "    @arg ax - axis where to plot\n",
    "    \n",
    "    to access specific latent variable \n",
    "    lat_variables['0'][1][:,2] # cluster 0, going to target 1, latent variable 2 (: indicates all time steps)\n",
    "    '''\n",
    "    lat_var = []\n",
    "    num_targets = np.shape(target_index)[0]\n",
    "\n",
    "    for target in range(num_targets):\n",
    "        lat_var.append(lat_variables[f'{cluster}'][target][:,l_var]) # latent variable for one cluster including all targets\n",
    "\n",
    "    # get all altent variables for one cluster\n",
    "    for i in range(len(lat_var)): # for each target\n",
    "        ax.plot(lat_var[i][:],label=f'target {i}') \n",
    "    #ax.legend(bbox_to_anchor=(0.5,-0.05))\n",
    "    ax.set_title(f'cluster {cluster}, latent variable {l_var}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de9c6e-367f-4077-a8d0-aa07c51cafdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Firing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659935f4-1af2-4b27-8006-2cc091e71dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firing_rate_per_cluster(td_processed, clustering_solution):\n",
    "    '''\n",
    "    calculate firing rate of each neuron and average firing rate of each cluster\n",
    "    \n",
    "    @arg td_processed - preprcessed SPIKES data (not rates)\n",
    "    @clustering_solution - list of clusters each neuron belongs to\n",
    "    \n",
    "    Note SME = standard error of a sample mean\n",
    "    \n",
    "    @return overall_FR - average firing rate across all neurons and all trials\n",
    "    @return overall_FR_sme - SME  across all neurons and all trials\n",
    "    @return avg_FR - average firing rate of each cluster\n",
    "    @return sme_FR - SME of firing rate of each cluster\n",
    "    @return max_FR - max firing rate of each cluster\n",
    "    '''\n",
    "    \n",
    "    #fr_neurons -> avergae firing rate of each neuron across all trials [fr_neuron0, fr_neuron1, ..., fr_neuronN]\n",
    "    # shape (N, ) where N is the number of neurons in signal - inlcuding M1 and PMd in total 309 neurons\n",
    "    fr_neurons = get_average_firing_rates(td_processed, \"both_spikes\",divide_by_bin_size=None) \n",
    "    overall_FR = np.mean(fr_neurons)\n",
    "    overall_FR_sme = np.std(fr_neurons)/np.sqrt(len(fr_neurons)) \n",
    "\n",
    "    num_clusters = len(set(clustering_solution))\n",
    "    avg_FR = np.zeros((num_clusters,1)) # average firing rate of each cluster\n",
    "    max_FR = np.zeros((num_clusters,1)) # max firing rate of each cluster\n",
    "    sme_FR = np.zeros((num_clusters,1)) # sme firing rate of each cluster = std/sqrt(N)\n",
    "    \n",
    "    for clust in range(num_clusters): # for each cluster\n",
    "        fr_cluster = []\n",
    "        for neuron in range(len(clustering_solution)): # for each neuron\n",
    "            if(clustering_solution[neuron] == clust): # if neuron is in cluster 'clust'\n",
    "                fr_cluster.append(fr_neurons[neuron]) # append firing rate of neuron\n",
    "\n",
    "        fr_cluster = np.array(fr_cluster)\n",
    "        avg_FR[clust] = np.mean(fr_cluster)\n",
    "        sme_FR[clust] = np.std(fr_cluster)/math.sqrt(len(fr_cluster))\n",
    "        max_FR[clust] = np.amax(fr_cluster)\n",
    "    \n",
    "    return overall_FR, overall_FR_sme, avg_FR, sme_FR, max_FR\n",
    "\n",
    "\n",
    "def plot_FR(umap_df, overall_FR, overall_FR_sme, avg_FR, sme_FR, max_FR, plot_umap=True, plot_fr_cluster=True):\n",
    "    '''\n",
    "    plot firing rate on UMAP graph and plot firing rate for each cluster\n",
    "    \n",
    "    @arg umap_df - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                    graph and corresponding x and y position\n",
    "    @arg overall_FR - average firing rate among all neurons\n",
    "    @arg overall_FR_sme - SME of firing rate among all neurons\n",
    "    @arg avg_FR - average firing rate of each cluster\n",
    "    @arg sme_FR - SME of average firing rate of each cluster\n",
    "    @arg max_FR - max firing rate of each cluster\n",
    "    @arg plot_umap - if True plot firing rate of each neuron on UMAP graph\n",
    "    @arg plot_fr_cluster - if True plot average firing rate of each cluster separately\n",
    "    '''\n",
    "    \n",
    "    if plot_umap:\n",
    "        # plot firing rate on UMAP plot\n",
    "        fr_neurons = get_average_firing_rates(td_processed, \"both_spikes\",divide_by_bin_size=None) \n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(8, 5)\n",
    "        feature_label = 'Average FR'\n",
    "        cmap = sns.color_palette('crest', as_cmap=True)\n",
    "        scat = plt.scatter(umap_df['x'].tolist(), umap_df['y'].tolist(),alpha = 0.8, c = fr_neurons, cmap = cmap)\n",
    "        cax = fig.add_axes([0.1, 0.05, 0.8, 0.03])\n",
    "        cbar = fig.colorbar(scat, cax=cax, orientation='horizontal')\n",
    "        cbar.set_label(feature_label,labelpad=10,fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.show()\n",
    "    \n",
    "    if plot_fr_cluster:\n",
    "        f, arr = plt.subplots(1,2)\n",
    "        f.set_size_inches(17,5)\n",
    "        x_pos = np.arange(len(max_FR))\n",
    "        arr[0].bar(x_pos,max_FR[:,0], color = ['#5e60ce','#00c49a','#ffca3a','#D81159','#fe7f2d','#7bdff2','#0496ff','#efa6c9'], yerr=sme_FR[:,0] )\n",
    "        arr[0].spines['right'].set_visible(False)\n",
    "        arr[0].spines['top'].set_visible(False)\n",
    "        arr[0].set_ylabel('Max FR (spikes/s)', fontsize=15)\n",
    "        arr[0].set_xlabel('Cluster')\n",
    "        \n",
    "        avg_fr_tot = np.append(avg_FR[:,0], overall_FR)\n",
    "        sme_fr_tot = np.append(sme_FR[:,0], overall_FR_sme)\n",
    "        \n",
    "        x_pos_fr = np.arange(len(avg_FR)+1)\n",
    "        arr[1].bar(x_pos_fr, avg_fr_tot, color = ['#5e60ce','#00c49a','#ffca3a','#D81159','#fe7f2d','#7bdff2','#0496ff','#efa6c9'], yerr=sme_fr_tot)\n",
    "        arr[1].spines['right'].set_visible(False)\n",
    "        arr[1].spines['top'].set_visible(False)\n",
    "        arr[1].set_ylabel('Average FR (spikes/s)',fontsize=15)\n",
    "        arr[1].set_xlabel('Cluster (7th is baseline -> avg firing rate of all neurons)')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d05eb-7e37-45f4-9e5e-a9c5d0f29c0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Peak to trough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85dce891-79ff-4f24-ae0f-53bd8e62d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ptt(waveform):\n",
    "    '''\n",
    "    measure peak to trough duration on a given waveform\n",
    "    @arg waveform  - list containing one waveform data\n",
    "    \n",
    "    @return d - peak to trough duration\n",
    "    '''\n",
    "    min_value = np.min(waveform)\n",
    "    index_min = np.where(waveform == min_value) # index of min -> argmin\n",
    "\n",
    "    max_value = np.max(waveform[index_min[0][0]:]) # from index of min and after - check index1 dimensions\n",
    "    index_max = np.where(waveform[index_min[0][0]:] == np.max(waveform[index_min[0][0]:])) # argmax\n",
    "    index_max[0][0] = index_max[0][0] + index_min[0][0]\n",
    "\n",
    "    d = (index_max[0][0] - index_min[0][0])\n",
    "    return d\n",
    "            \n",
    "def peak_to_trough_per_cluster(umap_df, neuron_types):\n",
    "    '''\n",
    "    find min point of the waveform and the max after that point\n",
    "    get their indices so that the time duration between them can be calculated -> peak-to-trough duration\n",
    "    \n",
    "    @arg umap_df - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                    graph and corresponding x and y position\n",
    "    @arg neuron_types - dict: key is cluster, values are list with all neurons belonging to that cluster\n",
    "    \n",
    "    @return distance_mean - average ptt for each cluster \n",
    "    @return distance_std - std of ptt for each cluster \n",
    "    @return sme - SME of ptt for each cluster \n",
    "    '''\n",
    "    \n",
    "    ptt_mean = [] # list containing avergae peak-to-trough distance for each cluster\n",
    "    ptt_std = [] # list containing std peak-to-trough distance for each cluster\n",
    "    ptt_sme = [] # list containing std/sqrt(n) peak-to-trough distance for each cluster\n",
    "\n",
    "\n",
    "    # peak to trough for all neurons ordered by clusters\n",
    "    for cluster in range(len(neuron_types)): # for each cluster\n",
    "        group_waveforms = umap_df.iloc[neuron_types[cluster]]['waveform'].tolist() # waveform data\n",
    "        distance = [] # list containing peak-to-trough distance for each neuron\n",
    "\n",
    "        for i in range(len(group_waveforms)): # for each neuron in this cluster\n",
    "            waveform = group_waveforms[i]\n",
    "            d = get_ptt(waveform) * 0.03 # each bin is 0.03sec\n",
    "            distance.append(d)\n",
    "\n",
    "        distance = np.array(distance)\n",
    "        \n",
    "        d_mean = np.mean(distance)\n",
    "        ptt_mean.append(d_mean)\n",
    "        \n",
    "        d_std = np.std(distance)\n",
    "        ptt_std.append(d_std)\n",
    "        \n",
    "        sme_cluster = np.std(distance)/math.sqrt(len(distance))\n",
    "        ptt_sme.append(sme_cluster)\n",
    "\n",
    "    return ptt_mean, ptt_std, ptt_sme\n",
    "\n",
    "\n",
    "def peak_to_trough_all(data_normalized):\n",
    "    '''\n",
    "    compute peak to trough of all neurons (in the order as found in the dataset)\n",
    "    \n",
    "    @arg data_normalized - list of all normalized waveforms\n",
    "    @return list of peak to trough of each neuron\n",
    "    '''\n",
    "\n",
    "    ptt_overall = [] # peak to trough across all neurons\n",
    "    \n",
    "    for i in range(len(data_normalized)):\n",
    "        waveform = data_normalized[i]\n",
    "        d = get_ptt(waveform) * 0.03 # each bin is 0.03sec\n",
    "        ptt_overall.append(d)\n",
    "        \n",
    "    ptt_overall = np.array(ptt_overall)\n",
    "    \n",
    "    return ptt_overall\n",
    "\n",
    "def plot_ptt(umap_df, ptt_overall, ptt_mean, ptt_sme, plot_umap=True, plot_ptt_cluster=True):\n",
    "    '''\n",
    "    plot peak-to-trough on UMAP graph and plot peak-to-trough for each cluster\n",
    "    \n",
    "    @arg umap_df - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                    graph and corresponding x and y position\n",
    "    @arg ptt_overall - ptt of each neuron\n",
    "    @arg ptt_mean - average peak-to-trough of each cluster\n",
    "    @arg ptt_sme - SME of peak-to-trough of each cluster\n",
    "    @arg plot_umap - if True plot peak-to-trough of each neuron on UMAP graph\n",
    "    @arg plot_ptt_cluster - if True plot average peak-to-trough of each cluster separately\n",
    "    '''\n",
    "    \n",
    "    if plot_umap:\n",
    "        # plot on UMAP graph\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        feature_label = 'Peak to trough distance [ms]'\n",
    "        cmap = sns.color_palette('crest', as_cmap=True)\n",
    "        scat = plt.scatter(umap_df['x'].tolist(), umap_df['y'].tolist(),alpha = 0.8,\n",
    "                            c = ptt_overall, cmap = cmap)\n",
    "        cax = fig.add_axes([0.1, 0.05, 0.8, 0.03])\n",
    "        cbar = fig.colorbar(scat, cax=cax, orientation='horizontal')\n",
    "        cbar.set_label(feature_label,labelpad=10,fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "    if plot_ptt_cluster:\n",
    "        # compare average peak-to-trough between clusters\n",
    "        ptt_mean = np.array(ptt_mean)\n",
    "        ptt_sme = np.array(ptt_sme)\n",
    "        \n",
    "        ptt_overall_sme = np.std(ptt_overall)/math.sqrt(len(ptt_overall))\n",
    "        ptt_overall_mean = np.mean(ptt_overall)\n",
    "        \n",
    "        mean_ptt = np.append(ptt_mean, ptt_overall_mean)\n",
    "        sme_ptt = np.append(ptt_sme, ptt_overall_sme)\n",
    "        \n",
    "        x_pos_ptt = np.arange(len(mean_ptt))\n",
    "\n",
    "\n",
    "        f, arr = plt.subplots(figsize=(5, 4))\n",
    "        arr.bar(x_pos_ptt, mean_ptt, color = ['#5e60ce','#00c49a','#ffca3a','#D81159','#fe7f2d','#7bdff2','#0496ff','#efa6c9'],\n",
    "                yerr=sme_ptt)\n",
    "        arr.spines['right'].set_visible(False)\n",
    "        arr.spines['top'].set_visible(False)\n",
    "        arr.set_ylabel('Peak to trough duration [ms]', fontsize = 15)\n",
    "        arr.set_xlabel('Cluster (last is population baseline)')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a459dd2-788e-46ab-b6ce-a880e34c3367",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## AP width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95679101-1dac-4eea-a074-ab68ec5bfbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AP_width(waveform):\n",
    "    '''\n",
    "    compute AP width of given waveform data\n",
    "    @arg waveform - waveform data\n",
    "    \n",
    "    @return AP width of the given waveform data\n",
    "    '''\n",
    "    # the first point is the max point before the global minimum\n",
    "    # the second one is the global minimum\n",
    "    # the third one is the max after the global minimum\n",
    "    # calculate index of all three points\n",
    "    \n",
    "    # global min of waveform\n",
    "    min_value = np.amin(waveform)\n",
    "    index_min = np.where(waveform == np.amin(waveform[:30])) # index of global min\n",
    "    index_min = index_min[0][0]\n",
    "\n",
    "    # max before global min\n",
    "    max_value = np.amax(waveform[:index_min])\n",
    "    index_max = np.where(waveform[:index_min] == np.amax(waveform[:index_min]))\n",
    "    index_max = index_max[0][0]\n",
    "                                  \n",
    "    # max after global min\n",
    "    indexMax = np.where(waveform[index_min:] == np.amax(waveform[index_min:]))\n",
    "    indexMax = indexMax[0][0] + index_min\n",
    "    \n",
    "    height = (max_value + min_value)/2\n",
    "    found_both = 0\n",
    "    for index in range(index_max, indexMax):\n",
    "        if found_both == 2:\n",
    "            break\n",
    "        if waveform[index] > height and waveform[index+1] < height:\n",
    "            index1 = index\n",
    "            found_both += 1\n",
    "        if waveform[index] < height and waveform[index+1] > height:\n",
    "            index2 = index\n",
    "            found_both += 1\n",
    "\n",
    "    width = index2 - index1\n",
    "    \n",
    "    # plot only to check it's correct\n",
    "    '''\n",
    "    plt.plot(waveform, '.')\n",
    "    plt.grid()\n",
    "    plt.hlines(height, index1, index2, color='k')\n",
    "    plt.show()\n",
    "    '''\n",
    "    return width\n",
    " \n",
    "    \n",
    "def AP_width_per_cluster(umap_df, neuron_types):\n",
    "    '''\n",
    "    find average AP width of each cluster\n",
    "    \n",
    "    @arg umap_df - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                    graph and corresponding x and y position\n",
    "    @arg neuron_types - dict: key is cluster, values are list with all neurons belonging to that cluster\n",
    "    \n",
    "    @return widths_mean - average AP width for each cluster \n",
    "    @return widths_sme - SME of AP width for each cluster \n",
    "    '''\n",
    "    widths_mean = []\n",
    "    widths_sme = []\n",
    "\n",
    "    # AP width for all neurons ordered by clusters\n",
    "    for cluster in range(len(neuron_types)): # for each cluster\n",
    "        group_waveforms = umap_df.iloc[neuron_types[cluster]]['waveform'].tolist() # all (average) waveforms in this cluster\n",
    "        widths_cluster = []\n",
    "\n",
    "        for i in range(len(group_waveforms)): # for each neuron in this cluster\n",
    "            waveform = group_waveforms[i]\n",
    "            width = get_AP_width(waveform) * 0.03 # in waveform data each time bin is 0.03 sec\n",
    "            widths_cluster.append(width)\n",
    "\n",
    "        widths_mean.append(np.mean(widths_cluster))\n",
    "        widths_sme.append(np.std(widths_cluster)/math.sqrt(len(group_waveforms)))\n",
    "    \n",
    "    return widths_mean, widths_sme\n",
    "\n",
    "\n",
    "def AP_width_all(data_normalized):\n",
    "    '''\n",
    "    compute AP width of all neurons (in the order as found in the dataset)\n",
    "    \n",
    "    @arg data_normalized - list of all normalized waveforms\n",
    "    @return list of AP width of each neuron\n",
    "    '''\n",
    "    AP_all = [] # AP width across all neurons\n",
    "\n",
    "    for i in range(len(data_normalized)):\n",
    "        waveform = data_normalized[i]\n",
    "        width = get_AP_width(waveform) * 0.03 # in waveform data each time bin is 0.03 sec\n",
    "        AP_all.append(width)\n",
    "        \n",
    "    AP_all = np.array(AP_all)\n",
    "\n",
    "    return AP_all\n",
    "\n",
    "\n",
    "def plot_AP_width(umap_df, AP_all, widths_mean, widths_sme, plot_umap=True, plot_AP_cluster=True):\n",
    "    '''\n",
    "    plot Action Potential width on UMAP graph and plot AP width for each cluster\n",
    "    \n",
    "    @arg umap_df - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                    graph and corresponding x and y position\n",
    "    @arg AP_all - AP width of each neuron\n",
    "    @arg widths_mean - average AP width of each cluster\n",
    "    @arg widths_sme - SME of AP width of each cluster\n",
    "    @arg plot_umap - if True plot AP width of each neuron on UMAP graph\n",
    "    @arg plot_AP_cluster - if True plot average AP width of each cluster separately\n",
    "    '''\n",
    "    \n",
    "    if plot_umap:\n",
    "        # plot on UMAP graph\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        feature_label = 'Action potential width [ms]'\n",
    "        cmap = sns.color_palette('crest', as_cmap=True)\n",
    "        scat = plt.scatter(umap_df['x'].tolist(), umap_df['y'].tolist(),alpha = 0.8,\n",
    "                            c = AP_all,\n",
    "                            cmap = cmap )\n",
    "        cax = fig.add_axes([0.1, 0.05, 0.8, 0.03])\n",
    "        cbar = fig.colorbar(scat, cax=cax, orientation='horizontal')\n",
    "        cbar.set_label(feature_label,labelpad=10,fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "    if plot_AP_cluster:\n",
    "        # compare average AP width between clusters\n",
    "        AP_all_sme = np.std(AP_all)/math.sqrt(len(AP_all))\n",
    "        AP_all_mean = np.mean(AP_all)\n",
    "        \n",
    "        x_pos_ap = np.arange(len(widths_mean)+1)\n",
    "        AP_widths_mean = np.append(widths_mean, AP_all_mean)\n",
    "        AP_widths_sme = np.append(widths_sme, AP_all_sme)\n",
    "\n",
    "        f, arr = plt.subplots(figsize=(5, 4))\n",
    "        arr.bar(x_pos_ap, AP_widths_mean, color = ['#5e60ce','#00c49a','#ffca3a','#D81159','#fe7f2d','#7bdff2','#0496ff','#efa6c9'],\n",
    "                yerr=AP_widths_sme)\n",
    "        arr.spines['right'].set_visible(False)\n",
    "        arr.spines['top'].set_visible(False)\n",
    "        arr.set_ylabel('Average AP width [ms]', fontsize = 15)\n",
    "        arr.set_xlabel('Cluster (last is population baseline)')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4cb6e-fd68-4403-b367-99afd9358620",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plot violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1e10b-e40d-4f38-9daf-678758284e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin(neurons_prop, neuron_types, color, ylabel, shift=0, ax=False):\n",
    "    '''\n",
    "    plot distribution of physiologival property as violin plots per each cluster\n",
    "    \n",
    "    @arg neurons_props - physiological property of each neuron in the dataset\n",
    "    @arg neuron_types - dict: key is cluster, values are list with all neurons belonging to that cluster\n",
    "    @color - list of colors to plot each cluster\n",
    "    @arg ylabel - ylabel for plot\n",
    "    @arg shift - not equal 0 if need to compare 2 datasets on the same plot\n",
    "    @arg ax - axes where to plot graph, if False, generate new axes\n",
    "    '''\n",
    "    _neurons_cluster = []\n",
    "\n",
    "    # select only neurons from specific cluster\n",
    "    for clust in range(len(neuron_types)):\n",
    "        _neurons_cluster.append(neurons_prop[neuron_types[clust]]) # mean ptt\n",
    "\n",
    "    if ax == False:\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "    alpha = 0.6\n",
    "    if shift < 0: alpha = 0.2\n",
    "    \n",
    "    pos = 1\n",
    "    if shift != 0: pos = 1.5\n",
    "    \n",
    "    for clust in range(len(neuron_types)):\n",
    "        violin = ax.violinplot(_neurons_cluster[clust], positions=[pos*clust+shift], showmedians=True)\n",
    "       # violin['bodies'].set_facecolor(color[clust])\n",
    "        violin['bodies'][0].set_facecolor(color[clust])\n",
    "        violin['cbars'].set_colors(color[clust])\n",
    "        violin['cmedians'].set_colors(color[clust])\n",
    "        violin['cmins'].set_colors(color[clust])\n",
    "        violin['cmaxes'].set_colors(color[clust])\n",
    "        violin['bodies'][0].set_alpha(alpha)\n",
    "\n",
    "\n",
    "    clust += 1\n",
    "    violin = ax.violinplot(neurons_prop, positions=[pos*clust+shift], showmedians=True)\n",
    "    violin['bodies'][0].set_facecolor('k')\n",
    "    violin['cbars'].set_colors('k')\n",
    "    violin['cmedians'].set_colors('k')\n",
    "    violin['cmins'].set_colors('k')\n",
    "    violin['cmaxes'].set_colors('k')\n",
    "    violin['bodies'][0].set_alpha(alpha)\n",
    "    ax.set_xticks([i*pos for i in [0,1,2,3,4]], labels = ['0','1','2','3','Population']);\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c2566-830a-4c18-9c29-7d048e7e9807",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011badc-45d2-4007-bc86-1fab862c1ee4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17536698-ae41-4a9f-81eb-fd5db592cb2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(X, y, training_range, testing_range, valid_range, bins_before, bins_current, bins_after, flat, zero_center):\n",
    "    '''\n",
    "    split data into training, test and validation\n",
    "    \n",
    "    @arg X - input data: spikes with history\n",
    "    @arg y - output data (what needs to be predicted)\n",
    "    @arg training_range - part of data used for training\n",
    "    @arg testing_range - part of data used for testing (used to find optimal parameters)\n",
    "    @arg valid_range - part of data used for validation\n",
    "    @arg bins_before - how many bins of neural data prior to the output are used for decoding\n",
    "    @arg bins_current - whether to use concurrent time bin of neural data\n",
    "    @arg bins_after - how many bins of neural data after the output are used for decoding\n",
    "    @arg flat - if True, flatten input data so so each \"neuron / time\" is a single feature \n",
    "                 format for Wiener Filter, Wiener Cascade, XGBoost, Dense Neural Network and GBN\n",
    "    @arg zero_center - if True, compute zer-centered data\n",
    "\n",
    "    @return input and output data split into train, test and validation data\n",
    "    '''\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    #Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "    #This makes it so that the different sets don't include overlapping neural data\n",
    "    training_set = np.arange(int(np.round(training_range[0]*num_examples))+bins_before, int(np.round(training_range[1]*num_examples))-bins_after)\n",
    "    testing_set = np.arange(int(np.round(testing_range[0]*num_examples))+bins_before, int(np.round(testing_range[1]*num_examples))-bins_after)\n",
    "    valid_set = np.arange(int(np.round(valid_range[0]*num_examples))+bins_before, int(np.round(valid_range[1]*num_examples))-bins_after)\n",
    "\n",
    "\n",
    "    if flat: # flatten\n",
    "        X = X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "    #Get training data\n",
    "    y_train = y[training_set,...]\n",
    "    X_train = X[training_set,...]\n",
    "\n",
    "    #Get testing data\n",
    "    y_test = y[testing_set,...]\n",
    "    X_test = X[testing_set,...]\n",
    "\n",
    "    #Get validation data\n",
    "    y_valid = y[valid_set,...]\n",
    "    X_valid = X[valid_set,...]\n",
    "    \n",
    "    #Z-score \"X\" inputs. \n",
    "    X_train_mean = np.nanmean(X_train,axis=0)\n",
    "    X_train_std = np.nanstd(X_train,axis=0)\n",
    "    X_train = (X_train-X_train_mean)/X_train_std\n",
    "    X_test = (X_test-X_train_mean)/X_train_std\n",
    "    X_valid = (X_valid-X_train_mean)/X_train_std\n",
    "    \n",
    "    if zero_center:\n",
    "        #Zero-center outputs\n",
    "        y_train_mean = np.mean(y_train,axis=0)\n",
    "        y_train = y_train-y_train_mean\n",
    "        y_test = y_test-y_train_mean\n",
    "        y_valid = y_valid-y_train_mean\n",
    "        \n",
    "        \n",
    "        \n",
    "    return X_train, X_test, X_valid, y_train, y_test, y_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c536642-88c6-4a0c-858b-f66522128c4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## R2 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae60098-b2be-4069-b45e-cab5ec520e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_R2(r2, r2_all, color='k'):\n",
    "    '''\n",
    "    plot average coefficient R2 of each cluster and average R2 when sampled neurons from all dataset\n",
    "    \n",
    "    @arg r2 - R2 from each cluster, list NxM, N is number of repetitions, M is number of clusters\n",
    "    @arg r2_all - list of R2 obtained randomly sampling neurons, size is 1xN\n",
    "    @arg color - color of the plotted line, default is balck\n",
    "    \n",
    "    @return average R2 and SME\n",
    "    '''\n",
    "    # R2 from each cluster\n",
    "    r2_clust_mean = np.mean(r2, axis=0)\n",
    "    r2_clust_sme = np.std(r2, axis=0)/math.sqrt(np.shape(r2)[0])\n",
    "\n",
    "    # R2 sampling neurons randomly from dataset\n",
    "    r2_all_avg = np.mean(r2_all)\n",
    "    r2_all_sme = np.std(r2_all)/math.sqrt(len(r2_all))\n",
    "\n",
    "    r2_mean = np.append(r2_clust_mean, r2_all_avg)\n",
    "    r2_sme = np.append(r2_clust_sme, r2_all_sme)\n",
    "\n",
    "    clusts = np.arange(np.shape(r2)[1]+1)\n",
    "    plt.errorbar(clusts, r2_mean, yerr=r2_sme,c=color, marker='o', fillstyle='full', markerfacecolor=color)\n",
    "    #plt.xlabel('Cluster (last is whole dataset baseline)', fontsize=12)\n",
    "    plt.ylabel(r'Coefficient of determination, $R^2$', fontsize=12)\n",
    "    \n",
    "    return r2_mean, r2_sme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c25d4-6d2b-4a6c-8aef-f0f7304e60b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Velocity decoding vs cluster properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e08aa0-063f-4c1a-b850-af5271600ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_R2_vs_property(r2_mean, r2_sme, avg, sme, CUSTOM_PAL_SORT, y_label):\n",
    "    '''\n",
    "    plot R2 coefficient from velocity decoding of each cluster vs firing rate\n",
    "    \n",
    "    @arg r2_mean - average R2 score for each cluster\n",
    "    @arg r2_std - std of R2 for each cluster\n",
    "    @arg avg_fr - average firing rate of each cluster\n",
    "    @arg sme_fr - SME of firing rate of each cluster\n",
    "    @arg CUSTOM_PAL_SORT_3 - colour palette \n",
    "    @arg y_label - ylabel for plot, 'Coefficient of Determination' or 'Target Classifier Accuracy'\n",
    "    '''\n",
    "    f, ax2 = plt.subplots(figsize=(len(r2_mean),5))\n",
    "    \n",
    "    clusts = np.arange(len(r2_mean))\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.errorbar(clusts, r2_mean, yerr=r2_sme, marker='o', fillstyle='full', markerfacecolor='w', color='k', ecolor='k')\n",
    "    ax1.set_ylabel(r'Coefficient of determination, $R^2$')\n",
    "    ax1.yaxis.label.set_color('k')\n",
    "    ax1.tick_params(axis='y',colors='k')\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['left'].set_color('k')\n",
    "    \n",
    "    x_pos = np.arange(len(avg))\n",
    "    ax2.bar(x_pos, avg, color = CUSTOM_PAL_SORT, yerr=sme, ecolor='gray')\n",
    "    ax2.set_xlabel('Cluster (last is population baseline)')\n",
    "    ax2.set_ylabel(f'{y_label}',fontsize=12)\n",
    "    ax2.tick_params(axis='y')\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e1fc9-bc0f-41c7-bf78-394931d9fb69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# M1 vs PMd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc71bb-cf75-497d-b94d-9670dcaea987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used\n",
    "def target_classifier_M1_and_PMd(td_processed, M1_neurons, PMd_neurons, N_samples, pca_dims, rep):\n",
    "    '''\n",
    "    implement target classifier on data sampled from M1 and PMd separately\n",
    "    @arg td_processed - preprocessed and filtered firing data\n",
    "    @arg M1_neurons - int, number of neurons from M1\n",
    "    @arg PMd_neurons - int, number of neurons from PMd\n",
    "    @arg N_samples - number of neurons sampled from M1 or PMd\n",
    "    @arg pca_dims - number of dimensions for PCA\n",
    "    @arg rep - number of times to compute analysis with same parameters. At the end compute average\n",
    "    \n",
    "    @return accuracies - accuracy obtain for M1 and PMd, list size Nx2, N is number of repetitions\n",
    "    @return y_valid - true target values (not used)\n",
    "    @return y_predicitons - predeicted target values (not used)\n",
    "    '''\n",
    "    accuracies = np.zeros((rep,2))\n",
    "\n",
    "    for neurons in ['M1', 'PMd']:\n",
    "        for z in range(rep):\n",
    "            print(neurons, z)\n",
    "\n",
    "            move_td = td_processed.copy()\n",
    "            y = []\n",
    "            neural_spikes = []\n",
    "\n",
    "            if neurons == 'M1':\n",
    "                pos = 0\n",
    "                sampled_data = random.sample(list(np.arange(M1_neurons)), N_samples)\n",
    "                for i in range(move_td.shape[0]):\n",
    "                    move_td.M1_rates[i] = move_td.M1_rates[i][:, sampled_data]\n",
    "                move_td = dim_reduce(move_td, PCA(pca_dims), \"M1_rates\", \"M1_pca\")\n",
    "                neural_data = concat_trials(move_td, \"M1_pca\")\n",
    "\n",
    "                for i in range(move_td.shape[0]):\n",
    "                    y.append(move_td.target_id[i])\n",
    "                    neural_spikes.append(move_td.M1_pca[i])\n",
    "\n",
    "\n",
    "            elif neurons == 'PMd':\n",
    "                pos = 1\n",
    "                sampled_data = random.sample(list(np.arange(PMd_neurons)), N_samples)\n",
    "                for i in range(move_td.shape[0]):\n",
    "                    move_td.PMd_rates[i] = move_td.PMd_rates[i][:, sampled_data]\n",
    "                move_td = dim_reduce(move_td, PCA(pca_dims), \"PMd_rates\", \"PMd_pca\")\n",
    "                neural_data = concat_trials(move_td, \"PMd_pca\")\n",
    "\n",
    "                for i in range(move_td.shape[0]):\n",
    "                    neural_spikes.append(move_td.PMd_pca[i])\n",
    "                    y.append(move_td.target_id[i])\n",
    "\n",
    "            # define what time period to use spikes from (with respect to the output)\n",
    "            bins_before = 6\n",
    "            bins_current = 1 \n",
    "            bins_after = 6\n",
    "\n",
    "            y = np.array(y)\n",
    "            X = np.array(neural_spikes)\n",
    "\n",
    "            training_range = [0, 0.7]\n",
    "            testing_range = [0.7, 0.85]\n",
    "            valid_range = [0.85,1]\n",
    "\n",
    "            X_train, X_test, X_valid, y_train, y_test, y_valid = split_data(X, y, training_range, testing_range, valid_range,\n",
    "                                                                             bins_before, bins_current, bins_after, flat=False, zero_center=False)\n",
    "\n",
    "            #Declare model\n",
    "            LSTM_classifier = LSTMClassification(units=100, dropout=0, num_epochs=10)\n",
    "\n",
    "            #Fit model\n",
    "            LSTM_classifier.fit(X_train, y_train)\n",
    "\n",
    "            #Get predictions\n",
    "            y_predicitons =  LSTM_classifier.predict(X_valid)\n",
    "\n",
    "            #Get metric of fit\n",
    "            accuracy = np.sum(y_predicitons == y_valid) / float(len(y_predicitons))\n",
    "            accuracies[z,pos] = accuracy\n",
    "            \n",
    "    return accuracies, y_valid, y_predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01047de5-7c7c-4c5d-aa32-686525bedc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_classifier_M1_or_PMd(td_processed, neurons, M1_neuorns, PMd_neurons, N_samples, pca_dims, rep):\n",
    "     '''\n",
    "    implement target classifier on data sampled from either M1 or PMd \n",
    "    \n",
    "    @arg td_processed - preprocessed and filtered firing data\n",
    "    @arg neurons - str: 'M1' or 'PMd', if compute analysis on M1 neurons or on PMd neurons\n",
    "    @arg M1_neurons - int, number of neurons from M1\n",
    "    @arg PMd_neurons - int, number of neurons from PMd\n",
    "    @arg N_samples - number of neurons sampled from M1 or PMd\n",
    "    @arg pca_dims - number of dimensions for PCA\n",
    "    @arg rep - number of times to compute analysis with same parameters. At the end compute average\n",
    "    \n",
    "    @return accuracies - accuracy obtain for M1 or PMd, list size N, N is number of repetitions\n",
    "    @return y_valid - true target values (not used)\n",
    "    @return y_predicitons - predeicted target values (not used)\n",
    "    '''\n",
    "    \n",
    "    accuracies = np.zeros((rep))\n",
    "    \n",
    "    print(pca_dims)\n",
    "    for z in range(rep):\n",
    "        print(neurons, z)\n",
    "\n",
    "        move_td = td_processed.copy()\n",
    "        y = []\n",
    "        neural_spikes = []\n",
    "\n",
    "        if neurons == 'M1':\n",
    "            sampled_data = random.sample(list(np.arange(M1_neurons)), N_samples)\n",
    "            for i in range(move_td.shape[0]):\n",
    "                move_td.M1_rates[i] = move_td.M1_rates[i][:, sampled_data]\n",
    "            move_td = dim_reduce(move_td, PCA(pca_dims), \"M1_rates\", \"M1_pca\")\n",
    "            neural_data = concat_trials(move_td, \"M1_pca\")\n",
    "\n",
    "            for i in range(move_td.shape[0]):\n",
    "                y.append(move_td.target_id[i])\n",
    "                neural_spikes.append(move_td.M1_pca[i])\n",
    "\n",
    "\n",
    "        elif neurons == 'PMd':\n",
    "            sampled_data = random.sample(list(np.arange(PMd_neurons)), N_samples)\n",
    "            for i in range(move_td.shape[0]):\n",
    "                move_td.PMd_rates[i] = move_td.PMd_rates[i][:, sampled_data]\n",
    "            move_td = dim_reduce(move_td, PCA(pca_dims), \"PMd_rates\", \"PMd_pca\")\n",
    "            neural_data = concat_trials(move_td, \"PMd_pca\")\n",
    "\n",
    "            for i in range(move_td.shape[0]):\n",
    "                neural_spikes.append(move_td.PMd_pca[i])\n",
    "                y.append(move_td.target_id[i])\n",
    "\n",
    "        # define what time period to use spikes from (with respect to the output)\n",
    "        bins_before = 6\n",
    "        bins_current = 1 \n",
    "        bins_after = 6\n",
    "\n",
    "        y = np.array(y)\n",
    "        X = np.array(neural_spikes)\n",
    "\n",
    "        training_range = [0, 0.7]\n",
    "        testing_range = [0.7, 0.85]\n",
    "        valid_range = [0.85,1]\n",
    "\n",
    "        X_train, X_test, X_valid, y_train, y_test, y_valid = split_data(X, y, training_range, testing_range, valid_range,\n",
    "                                                                         bins_before, bins_current, bins_after, flat=False, zero_center=False)\n",
    "\n",
    "        #Declare model\n",
    "        LSTM_classifier = LSTMClassification(units=100, dropout=0, num_epochs=10)\n",
    "\n",
    "        #Fit model\n",
    "        LSTM_classifier.fit(X_train, y_train)\n",
    "\n",
    "        #Get predictions\n",
    "        y_predicitons =  LSTM_classifier.predict(X_valid)\n",
    "\n",
    "        #Get metric of fit\n",
    "        accuracy = np.sum(y_predicitons == y_valid) / float(len(y_predicitons))\n",
    "        accuracies[z] = accuracy\n",
    "            \n",
    "    return accuracies, y_valid, y_predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55b810-1f6c-40f7-8935-78db558bbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movement_decoding_M1_or_PMd(td_processed, neurons, M1_neuorns, PMd_neurons, N_samples, pca_dims, repetitions):\n",
    "     '''\n",
    "    implement velocity decoder on data sampled from either M1 or PMd \n",
    "    \n",
    "    @arg td_processed - preprocessed and filtered firing data\n",
    "    @arg neurons - str: 'M1' or 'PMd', if compute analysis on M1 neurons or on PMd neurons\n",
    "    @arg M1_neurons - int, number of neurons from M1\n",
    "    @arg PMd_neurons - int, number of neurons from PMd\n",
    "    @arg N_samples - number of neurons sampled from M1 or PMd\n",
    "    @arg pca_dims - number of dimensions for PCA\n",
    "    @arg repetitions - number of times to compute analysis with same parameters. At the end compute average\n",
    "    \n",
    "    @return r2 - R2 obtain for M1 or PMd, list size N, N is number of repetitions\n",
    "    @return y_valid - true target values (not used)\n",
    "    @return y_predicitons - predeicted target values (not used)\n",
    "    '''\n",
    "    r2 = np.zeros((repetitions))\n",
    "    \n",
    "    print(pca_dims)\n",
    "    for rep in range(repetitions):\n",
    "        print(neurons, rep)\n",
    "\n",
    "        move_td = td_processed.copy()\n",
    "        y = []\n",
    "        neural_spikes = []\n",
    "\n",
    "        if neurons == 'M1':\n",
    "            sampled_data = random.sample(list(np.arange(M1_neurons)), N_samples)\n",
    "            for i in range(move_td.shape[0]):\n",
    "                move_td.M1_rates[i] = move_td.M1_rates[i][:, sampled_data]\n",
    "            move_td = dim_reduce(move_td, PCA(pca_dims), \"M1_rates\", \"M1_pca\")\n",
    "            neural_data = concat_trials(move_td, \"M1_pca\")\n",
    "\n",
    "\n",
    "\n",
    "        elif neurons == 'PMd':\n",
    "            sampled_data = random.sample(list(np.arange(PMd_neurons)), N_samples)\n",
    "            for i in range(move_td.shape[0]):\n",
    "                move_td.PMd_rates[i] = move_td.PMd_rates[i][:, sampled_data]\n",
    "            move_td = dim_reduce(move_td, PCA(pca_dims), \"PMd_rates\", \"PMd_pca\")\n",
    "            neural_data = concat_trials(move_td, \"PMd_pca\")\n",
    "\n",
    "            \n",
    "        \n",
    "        # define what time period to use spikes from (with respect to the output)\n",
    "        bins_before = 6\n",
    "        bins_current = 1 \n",
    "        bins_after = 6\n",
    "\n",
    "\n",
    "        X = get_spikes_with_history(neural_data, bins_before, bins_after, bins_current)\n",
    "        y = concat_trials(move_td, \"vel\")\n",
    "\n",
    "        training_range = [0, 0.7]\n",
    "        testing_range = [0.7, 0.85]\n",
    "        valid_range = [0.85,1]\n",
    "\n",
    "        X_train, X_test, X_valid, y_train, y_test, y_valid = split_data(X, y, training_range, testing_range, valid_range,\n",
    "                                                                         bins_before, bins_current, bins_after, flat=False, zero_center=False)\n",
    "\n",
    "        #Declare model\n",
    "        model_lstm = LSTMDecoder(units=400,dropout=0,num_epochs=5)\n",
    "\n",
    "        #Fit model\n",
    "        model_lstm.fit(X_train,y_train)\n",
    "\n",
    "        #Get predictions\n",
    "        y_valid_predicted =  model_lstm.predict(X_valid)\n",
    "\n",
    "        #Get metric of fit\n",
    "        R2s_lstm = get_R2(y_valid,y_valid_predicted)\n",
    "        r2[rep] = np.mean(R2s_lstm)\n",
    "            \n",
    "    return r2, y_valid, y_valid_predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de3d60-b21b-463c-8c75-090f69c3db51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8ad6a5-f142-4364-b1b5-9fe2e61bf35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_cluster(fr_correlation_avg, neuron_types, neurons_tot):\n",
    "    '''\n",
    "    sort pairwise correlation of all neurons by cluster\n",
    "    \n",
    "    @arg fr_correlation_avg - list NxN, where N is the number of neurons\n",
    "                            each entry is the correlation between neurons i and neuron j\n",
    "    @arg neuron_types - dict: key is cluster, values are list with all neurons belonging to that cluster\n",
    "    @arg neurons_tot - int, N, total number of neurons\n",
    "    \n",
    "    @return fr_correlation_ordered - list NxN, where N is the number of neurons\n",
    "                            each entry is the correlation between neurons i and neuron j\n",
    "                            neurons are ordered by clusters\n",
    "    '''\n",
    "    neurons_ordered = [] # contain all neuron indices ordered by cluster they belong to\n",
    "    for cluster in range(len(neuron_types)):\n",
    "        neurons_ordered.extend(neuron_types[cluster])\n",
    "\n",
    "    fr_correlation_ordered = np.zeros((neurons_tot, neurons_tot))\n",
    "    for idx1 in range(neurons_tot):\n",
    "        neuron1 = neurons_ordered[idx1]\n",
    "\n",
    "        for idx2 in range(neurons_tot):\n",
    "            neuron2 = neurons_ordered[idx2]\n",
    "            fr_correlation_ordered[idx1, idx2] = fr_correlation_avg[neuron1, neuron2]\n",
    "    \n",
    "    return fr_correlation_ordered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3af776-4ab3-4181-a86c-a5d924799e55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Compare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fc992-2530-432a-892f-ff6bfdfcd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_clusters(umap_df1, umap_df2, neuron_types1, neuron_types2, plot=True):\n",
    "    '''\n",
    "    compare waveform data to match clusters of 2 arrays calculating correlation\n",
    "    compue average waveform og each cluster and compute pairwise correlation between each pair of clusters\n",
    "    \n",
    "    @arg umap_df1 - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                    graph and corresponding x and y position (dataset1)\n",
    "    @arg umap_df1 - pandas DataFrame with normalised waveform data of each point in UMAP\n",
    "                    graph and corresponding x and y position (dataset2)\n",
    "    @arg neuron_types1 - dict: key is cluster, values are list with all neurons belonging to that cluster (dataset1)\n",
    "    @arg neuron_types2 - dict: key is cluster, values are list with all neurons belonging to that cluster (dataset2)\n",
    "    @arg plot - if True, plot heatmap table of correlation coefficients\n",
    "\n",
    "    @return correlation - list NxN, where N is the number of clusters\n",
    "                            each entry is pairwise correlation between cluster i and cluster j\n",
    "    @return maxValue - list size N, index of best matches\n",
    "    '''\n",
    "    # get average waveform of each cluster for each dataset\n",
    "    df1 = pd.DataFrame() # each column -> average waveform for each cluster\n",
    "    df2 = pd.DataFrame()\n",
    "    # each column is a different cluster, each row is a different point in time (values are the average waveform of that cluster)\n",
    "\n",
    "    for cluster in range(len(neuron_types1)):\n",
    "        clust_waveforms = umap_df1.iloc[neuron_types1[cluster]]['waveform'].tolist() # all waveforms in this cluster\n",
    "        df1[f'dataset 1 - cluster {cluster}'] = np.mean(clust_waveforms,axis=0) # mean waveform of each cluster\n",
    "\n",
    "        clust_waveforms = umap_df2.iloc[neuron_types2[cluster]]['waveform'].tolist() # all waveforms in this cluster\n",
    "        df2[f'dataset 2 - cluster {cluster}'] = np.mean(clust_waveforms,axis=0) # average waveform of each cluster\n",
    "\n",
    "    # compute correlation between clusters\n",
    "    correlation = np.zeros((len(neuron_types1), len(neuron_types2)))\n",
    "    for i in range(len(neuron_types1)):\n",
    "        for j in range(len(neuron_types2)):\n",
    "            correlation[i,j] = df1[df1.columns[i]].corr(df2[df2.columns[j]])\n",
    "\n",
    "    correlation = pd.DataFrame(correlation)\n",
    "\n",
    "    # create heatmap for the calculated correlation\n",
    "    if plot:\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.heatmap(correlation, annot=True, fmt='.2g')\n",
    "        #plt.title('Correlation between dataset 15/09/16 and 19/09/16', fontsize=14) # change with new dataset\n",
    "\n",
    "    # find max value of each row\n",
    "    maxValue = correlation.idxmax(axis=1) # max value of each row -> matching clusters\n",
    "    \n",
    "    return correlation, maxValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea0b384-26cf-4f7e-919b-121f7250106a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def order_array(arr_unordered, corr_idx, axis=None):\n",
    "    '''\n",
    "    reorder array to match order matched clusters\n",
    "    \n",
    "    @arg arr_unordered - unordered array, each column is a different cluster\n",
    "    @arg corr_idx - list containing order of matched clusters\n",
    "    @arg axis - str: 'col' or 'row',\n",
    "                if arr_unordered is a 2-dimensional array, need to specify order along which dimension\n",
    "    \n",
    "    \n",
    "    @return ordered array\n",
    "    '''\n",
    "    arr_unordered = np.array(arr_unordered)\n",
    "    arr_ordered = np.zeros((arr_unordered.shape))\n",
    "    for i, idx in enumerate(corr_idx):\n",
    "        if np.ndim(arr_unordered) == 2:\n",
    "            if axis == 'col':\n",
    "                arr_ordered[:,i] = arr_unordered[:,idx]\n",
    "            elif axis == 'row':\n",
    "                arr_ordered[i,:] = arr_unordered[idx,:]\n",
    "        else:\n",
    "            arr_ordered[i] = arr_unordered[idx]\n",
    "    return arr_ordered\n",
    "\n",
    "def compare_values_plot(ax, arr_mean1, arr_mean2, arr_sme1, arr_sme2, label, plot_sme=False):\n",
    "    clusts = np.arange(len(arr_mean1))\n",
    "    \n",
    "    dashed = (0, (5,5))\n",
    "    if plot_sme:\n",
    "        ax.errorbar(clusts, arr_mean1, yerr=arr_sme1, marker='o', fillstyle='full',\n",
    "                    color='gray', label=f'{label} 1',linestyle=dashed,markersize=5)\n",
    "        ax.errorbar(clusts, arr_mean2, yerr=arr_sme2, marker='o', fillstyle='full',\n",
    "                    color='darksalmon', label=f'{label} 2',linestyle=dashed, markersize=5)\n",
    "    \n",
    "    else:\n",
    "        ax.plot(arr_mean1, marker='o', color='gray', label=f'{label} 1',linestyle=dashed,markersize=5)\n",
    "        ax.plot(arr_mean2, marker='o', color='darksalmon', label=f'{label} 2',linestyle=dashed, markersize=5)\n",
    "    \n",
    "    ax.set_xlabel('Cluster')\n",
    "    ax.set_xticks([0,1,2,3,4], labels = ['0','1','2','3','Population']);\n",
    "    ax.spines['left'].set_linestyle(dashed)\n",
    "    ax.spines['top'].set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a28bf2-f618-496a-b602-9271bb8b3216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def div_circ_cmap(data, light_color, dark_color):\n",
    "    '''\n",
    "    generate colourmap that is divergent and circular\n",
    "    \n",
    "    @arg data - data to build colourmap on\n",
    "    @arg light_color, dark_color - colors in hex format\n",
    "    '''\n",
    "    \n",
    "    # Define the normalization range\n",
    "    minn = np.mean(data) - np.std(data)\n",
    "    maxx = np.mean(data) + np.std(data)\n",
    "    vmin, vmax = -np.max([abs(minn), maxx]), np.max([abs(minn), maxx])\n",
    "    #vmin, vmax = np.min(data), np.max(data)\n",
    "    #vmin, vmax = -1, 1\n",
    "\n",
    "    # Create a custom normalization that centers zero value\n",
    "    norm_out = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "\n",
    "    # Define the color map\n",
    "    cmap_name = 'diverging_circular'\n",
    "    num_steps = 256\n",
    "\n",
    "    # Define the color gradient for negative values (dark to light)\n",
    "    neg_colors = [dark_color, light_color]  # Dark to Light\n",
    "    neg_cmap = mcolors.LinearSegmentedColormap.from_list(cmap_name + '_neg', neg_colors, num_steps)\n",
    "\n",
    "    # Define the color gradient for positive values (light to dark)\n",
    "    pos_colors = [light_color, dark_color]  # Light to Dark\n",
    "    pos_cmap = mcolors.LinearSegmentedColormap.from_list(cmap_name + '_pos', pos_colors, num_steps)\n",
    "\n",
    "    # Combine the negative and positive colormaps into a circular colormap\n",
    "    cmap_out = mcolors.LinearSegmentedColormap.from_list(\n",
    "        cmap_name, [neg_cmap(0), 'white', pos_cmap(255)], num_steps\n",
    "    )\n",
    "    return cmap_out, norm_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa9ba0-305b-4710-b07b-d4d7c5328877",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Save to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a3932-6fae-4678-8c0e-8e52d7527d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save values into files\n",
    "def save_to_file(r2_vel_dataset1):\n",
    "    np.savetxt(f'{r2_vel_dataset1}.txt',r2_vel_dataset1)\n",
    "\n",
    "def load_from_file(file_name1, file_name2):\n",
    "    r2_vel_dataset1 = np.loadtxt(file_name1)\n",
    "    r2_all_dataset1 = np.mean(np.loadtxt(file_name2))\n",
    "    return r2_vel_dataset1, r2_all_dataset1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
